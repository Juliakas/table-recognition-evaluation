{"text": "Item Property Reason for Change or Deletion Clarity or relevance \u00a9 \u2014 Reported as not relevant by a large segment of the target population @ Generates an unacceptably large amount of mi: sing data points @ Generates many questions or requests for clarification from patients as they complete the PRO instrument \u00a9 Patients interpret items and responses in a way that is inconsistent with the PRO instrument\u2019s conceptual framework Response range \u00a9 A high percent of patients respond at the floor (response scale\u2019s worst end) or ceiling (response scale\u2019s optimal end) \u00a9 Patients note that none of the response choices applies to them \u00a9 Distribution of item responses is highly skewed Variability \u00a9 All patients give the same answer (i.e., no variance) \u00a9 Most patients choose only one response choice \u00a9 Differences among patients are not detected when important differences are known Reproducibility \u00a9 \u2014 Unstable scores over time when there is no logical reason for variation from one assessment to the next Inter-item correlation \u00a9 Item highly correlated (redundant) with other items in the same concept of interest Ability to detect change \u00a9 Item is not sensitive (i.e., does not change when there is a known change in the concepts of interest) Item discrimination \u00a9 Item is highly correlated with measures of concepts other than the one it is intended to measure @ \u2014 Item does not show variability in relation to some known population characteristics (i.e., severity level, classification of condition, or other known characteristic) Redundancy \u00a9 Item duplicates information collected with other items that have equal or better measurement properties Recall period \u00a9 \u2014 The population, disease state, or application of the instrument can affect the appropriateness of the recall period Measurement Type What Is Assessed? FDA Review Considerations Property Reliability Test-retest or intra- Stability of scores over time when no change | e \u2014 Intraclass correlation coefficient interviewer reliability (for is expected in the concept of interest e \u2014 Time period of assessment interviewer-administered PROs only) Internal consistency e Extent to which items comprising a scale Cronbach\u2019s alpha for summary scores measure the same concept \u00a2 \u2014 Item-total correlations e \u2014 Intercorrelation of items that contribute to a score e Internal consistency Inter-interviewer reliability Agreement among responses when the PRO | e __Interclass correlation coefficient (for interviewer-administered | is administered by two or more different PROs only) interviewers Validity Content validity Evidence that the instrument measures the Derivation of all items concept of interest including evidence from qualitative studies that the items and domains of an instrument are appropriate and comprehensive relative to its intended measurement concept, population, and use. Testing other measurement properties will not replace or rectify problems with content validity, Qualitative interview schedule Interview or focus group transcripts Items derived from the transcripts Composition of patients used to develop content Cognitive interview transcripts to evaluate patient understanding Construct validity Evidence that relationships among items, domains, and concepts conform to a priori hypotheses concerning logical relationships that should exist with measures of related concepts or scores produced in similar or diverse patient groups Strength of correlation testing a priori hypotheses (discriminant and convergent validity) Degree to which the PRO instrument can distinguish among groups hypothesized a priori to be different (known groups validity) Ability to detect change Evidence that a PRO instrument can identify differences in scores over time in individuals or groups (similar to those in the clinical trials) who have changed with respect to the measurement concept Within person change over time Effect size statistic", "cellCount": 81}