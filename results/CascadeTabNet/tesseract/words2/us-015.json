{"text": "Item Property\n\nReason for Change or Deletion\n\n \n\nClarity or relevance\n\nReported as not relevant by a large segment of the target population\nGenerates an unacceptably large amount of mi:\n\n \n\nsing data points\n\nGenerates many questions or requests for clarification from patients as they\ncomplete the PRO instrument\n\nPatients interpret items and responses in a way that is inconsistent with the\nPRO instrument\u2019s conceptual framework\n\n \n\nResponse range\n\nA high percent of patients respond at the floor (response scale\u2019s worst end)\nor ceiling (response scale\u2019s optimal end)\n\nPatients note that none of the response choices applies to them\nDistribution of item responses is highly skewed\n\n \n\nVariability\n\nAll patients give the same answer (i.e., no variance)\nMost patients choose only one response choice\n\nDifferences among patients are not detected when important differences are\nknown\n\n \n\nReproducibility\n\nUnstable scores over time when there is no logical reason for variation from\none assessment to the next\n\n \n\nInter-item correlation\n\nItem highly correlated (redundant) with other items in the same concept of\ninterest\n\n \n\nAbility to detect change\n\nItem is not sensitive (i.e., does not change when there is a known change in\nthe concepts of interest)\n\n \n\nItem discrimination\n\nItem is highly correlated with measures of concepts other than the one it is\nintended to measure\n\nItem does not show variability in relation to some known population\ncharacteristics (i.e., severity level, classification of condition, or other known\ncharacteristic)\n\n \n\n \n\n \n\nRedundancy Item duplicates information collected with other items that have equal or\nbetter measurement properties\nRecall period The population, disease state, or application of the instrument can affect the\n\n  \n\n \n\f  \n\n\u2014rr~_\u2014~Eoreese Ee *\u2122\n\n \n\nue\u201d\n\n   \n\n5A or\u2014eieeeeece\u2014erhr\u2014es ESO I iraaeceeee'~\n\n \n\nItem Property\n\nReason for Change or Deletion\n\n \n\nClarity or relevance\n\nReported as not relevant by a large segment of the target population\nGenerates an unacceptably large amount of mi:\n\n \n\nsing data points\n\nGenerates many questions or requests for clarification from patients as they\ncomplete the PRO instrument\n\nPatients interpret items and responses in a way that is inconsistent with the\nPRO instrument\u2019s conceptual framework\n\n \n\nResponse range\n\nA high percent of patients respond at the floor (response scale\u2019s worst end)\nor ceiling (response scale\u2019s optimal end)\n\nPatients note that none of the response choices applies to them\nDistribution of item responses is highly skewed\n\n \n\n \n\nVariability All patients give the same answer (i.e., no variance)\nMost patients choose only one response choice\nDifferences among patients are not detected when important differences are\nknown\n\nReproducibility Unstable scores over time when there is no logical reason for variation from\n\none assessment to the next\n\n \n\nInter-item correlation\n\nItem highly correlated (redundant) with other items in the same concept of\ninterest\n\n \n\nAbility to detect change\n\nItem is not sensitive (i.e., does not change when there is a known change in\nthe concepts of interest)\n\n \n\nItem discrimination\n\nItem is highly correlated with measures of concepts other than the one it is\nintended to measure\n\n \n\nItem does not show variability in relation to some known population\ncharacteristics (i.e., severity level, classification of condition, or other known\ncharacteristic)\n\n \n\n \n\n \n\nRedundancy Item duplicates information collected with other items that have equal or\nbetter measurement properties\nRecall period The population, disease state, or application of the instrument can affect the\n\n \n\nappropriateness of the recall period\n\n \n\n \n\f  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nMeasurement Type What Is Assessed? FDA Review Considerations\nProperty\nReliability Test-retest or intra- Stability of scores over time when no change | e \u2014 Intraclass correlation coefficient\n\ninterviewer reliability (for is expected in the concept of interest e \u2014 Time period of assessment\n\ninterviewer-administered\n\nPROs only)\n\nInternal consistency e Extent to which items comprising a scale Cronbach\u2019s alpha for summary scores\n\nmeasure the same concept \u00a2 \u2014 Item-total correlations\ne \u2014 Intercorrelation of items that contribute\n\nto a score\ne Internal consistency\n\nInter-interviewer reliability Agreement among responses when the PRO | e __Interclass correlation coefficient\n\n(for interviewer-administered | is administered by two or more different\n\nPROs only) interviewers\n\nValidity Content validity Evidence that the instrument measures the e Derivation of all items\nconcept of interest including evidence from \u00a9 = Qualitative interview schedule\nqualitative studies that the items and domains | e Interview or focus group transcripts\nof an instrument are appropriate and \u00a2 Items derived from the transcripts\ncomprehensive relative to its intended \u00a9 Composition of patients used to develop content\nMeasurement concept, population, and use. | \u00a2 Cognitive interview transcripts to evaluate patient\nTesting other measurement properties will understanding\nnot replace or rectify problems with content\nvalidity,\n\nConstruct validity Evidence that relationships among items, e Strength of correlation testing a priori hypotheses\ndomains, and concepts conform to a priori (discriminant and convergent validity)\nhypotheses concerning logical relationships | e Degree to which the PRO instrument can distinguish\nthat should exist with measures of related among groups hypothesized a priori to be different\nconcepts or scores produced in similar or (known groups validity)\ndiverse patient groups\n\nAbility to detect Evidence that a PRO instrument can identify | e\u00a2 Within person change over time\nchange differences in scores over time in individuals | \u00a2 \u2014 Effect size statistic\n\n \n\n \n\nor groups (similar to those in the clinical\ntrials) who have changed with respect to the\nmeasurement concent\n\n \n\n \n\n \n\f fable 2. ieasurement Froperties Considered in the Neview OF PNY instruments Used In Uilnical 1rials\n\n \n\n \n\n \n\n \n\n \n\nMeasurement Type What Is Assessed? FDA Review Considerations\nProperty\nReliability Test-retest or intra- Stability of scores over time when no change | e \u2014 Intraclass correlation coefficient\ninterviewer reliability (for is expected in the concept of interest e \u2014 Time period of assessment\ninterviewer-administered\nPROs only)\nInternal consistency e Extent to which items comprising a scale Cronbach\u2019s alpha for summary scores\nmeasure the same concept \u00a2 \u2014 Item-total correlations\ne \u2014 Intercorrelation of items that contribute\nto a score\ne Internal consistency\nInter-interviewer reliability Agreement among responses when the PRO | e __Interclass correlation coefficient\n(for interviewer-administered | is administered by two or more different\nPROs only) interviewers\nValidity Content validity Evidence that the instrument measures the Derivation of all items\n\nconcept of interest including evidence from\nqualitative studies that the items and domains\nof an instrument are appropriate and\ncomprehensive relative to its intended\nmeasurement concept, population, and use.\nTesting other measurement properties will\nnot replace or rectify problems with content\nvalidity,\n\nQualitative interview schedule\n\nInterview or focus group transcripts\n\nItems derived from the transcripts\n\nComposition of patients used to develop content\nCognitive interview transcripts to evaluate patient\nunderstanding\n\n \n\nConstruct validity\n\nEvidence that relationships among items.\ndomains, and concepts conform to a priori\nhypotheses concerning logical relationships\nthat should exist with measures of related\nconcepts or scores produced in similar or\ndiverse patient groups\n\nStrength of correlation testing a priori hypotheses\n(discriminant and convergent validity)\n\nDegree to which the PRO instrument can distinguish\namong groups hypothesized a priori to be different\n(known groups validity)\n\n \n\n \n\nAbility to detect\nchange\n\n \n\n \n\nEvidence that a PRO instrument can identify\ndifferences in scores over time in individuals\nor groups (similar to those in the clinical\ntrials) who have changed with respect to the\nmeasurement concept\n\n \n\nWithin person change over time\nEffect size statistic\n\n \n\n \n\f", "cellCount": 355}